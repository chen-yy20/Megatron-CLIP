{
  "train_micro_batch_size_per_gpu": 32,
  "gradient_accumulation_steps": 2,

  "optimizer": {
    "type": "AdamW",
    "params": {
      "torch_adam": false,
      "lr": 0.001,
      "betas": [
          0.8,
          0.999
      ],
      "eps": 1e-8,
      "weight_decay": 3e-7
    }
  },

  "steps_per_print" : 1,
  "wall_clock_breakdown" : true
}